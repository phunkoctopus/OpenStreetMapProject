{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Data Project: Sydney, Australia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sydney, New South Wales, Australia\n",
    "\n",
    "  • https://mapzen.com/data/metro-extracts/metro/sydney_australia/\n",
    "  \n",
    "  • https://www.openstreetmap.org/relation/5750005\n",
    "\n",
    "The map dataset chosen for this project is the Sydney, Australia metro area dataset from MapZen. I decided on this location because of a study abroad program in Australia I participated in during my undergrad. I lived in and traveled the country for six months, but was unable to trek the Harbour City. I have friends and family currently living in Sydney and would like to visit the area soon. In this project I'm looking to explore the dataset in an effort to contribute to the improvement of map data on the OpenStreetMap database website, gain some additional geographic familiarity and uncover some interesting facts about the city. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Sample Dataset\n",
    "\n",
    "Before auditing and cleaning the original dataset, I found it useful to create a smaller sample file in order to validate and work through any issues discovered. This allowed for a more streamlined process when writing, running, and correcting a number of python scripts and SQL queries. The sample file created was 6.9 MB in size and this process proved to be a time saver, especially in the case of the CSV conversion script. Once the code was suitable and successfully run on the sample file, it was applied to the original uncompressed OSM XML file and resulting tables created from the CSV conversion files.\n",
    "\n",
    "The method was utilized in assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity and ultimately the effort to clean and standardize the dataset. I identified several language errors in the location data including spelling, abbreviations, and typos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sampleconversion.py\n",
    "#Taking a systematic sample of elements from the original OSM region (Sydney, Australia).\n",
    "\n",
    "import xml.etree.ElementTree as ET  #Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"sydney_australia.osm\"  \n",
    "SAMPLE_FILE = \"sydney_sample.osm\"  #Create new file name for created sample.\n",
    "\n",
    "k = 50 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "    \n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Audits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Street Name Errors\n",
    "\n",
    "Python is used to audit and print out street names in the sample dataset, containing some unusual values. The function below is used to identify and correct unexpected spelling errors, street name abbreviations and typo inconsistencies. The function takes a string with street_name as an argument and returns the corrected name, iterating over each word in an address.\n",
    "\n",
    "In some cases mapping out more specific changes was required, especially considering the inimitable street names found nowhere else. A bit more verification was required in order to limit hypercorrections, using a popular map application. From the provided list the script audits and make changes to the 'mapping' variable in the OSM file. Therefore \"ApplegumCrescent\" is corrected to \"Applegum Crescent\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sydneyProcessing.py\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"sydney_australia.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$\\xf3', re.IGNORECASE)  \n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"West\", \"East\", \"South\", \"North\", \"Way\"]\n",
    "\n",
    "#Correct street name abbreviations and typo (i.e., Mainland St. => Mainland Street, \n",
    "#Broadway W => Broadway West, Boundary Rd. => Boundary Road) \n",
    "mapping = {\"St\": \"Street\",\n",
    "           \"St.\": \"Street\",\n",
    "           \"Rd.\": \"Road\",\n",
    "           \"Ave\": \"Avenue\",\n",
    "           \"west\": \"West\",\n",
    "           \"W\": \"West\",\n",
    "           \"street\": \"Street\",\n",
    "           \"Blvd\": \"Boulevard\",\n",
    "           \"Denmanstreet\": \"Denman Street\"\n",
    "           }\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "def update_name(name, mapping):    \n",
    "#makes revisions, updating street names based on mapping and what is expected.\n",
    "    m = street_type_re.search(name)\n",
    "    if m.group() not in expected:\n",
    "        if m.group() in mapping.keys():\n",
    "            name = re.sub(m.group(), mapping[m.group()], name)\n",
    "    return name\n",
    "\n",
    "\n",
    "def test():\n",
    "    st_types = audit(OSMFILE)\n",
    "\n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"=>\", better_name\n",
    "            if name == \"West Lexington St.\":\n",
    "                assert better_name == \"West Lexington Street\"\n",
    "            if name == \"Baldwin Rd.\":\n",
    "                assert better_name == \"Baldwin Road\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Code Audit\n",
    "\n",
    "Additional auditing was performed to verify correct use and consistency of the state code. The state code for New South Wales is NSW. The results of this audit show there were no inconsistencies.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scode.py\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"sydney_australia.osm\"\n",
    "state_type_re = re.compile(r'\\b\\S+\\.?$\\xf3', re.IGNORECASE)  \n",
    "\n",
    "expected = [\"NSW\"]\n",
    "\n",
    "\n",
    "def audit_state_type(state_types, state_code):\n",
    "    m = state_type_re.search(state_code)\n",
    "    if m:\n",
    "        state_type = m.group()\n",
    "        if state_type not in expected:\n",
    "            state_types[state_type].add(state_code)\n",
    "\n",
    "def is_state_code(elem):\n",
    "    return (elem.attrib['k'] == \"is_in:state_code\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    state_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_state_code(tag):\n",
    "                    audit_state_type(state_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return state_types\n",
    "    print state_types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SQL query performed once the CSVs have been loaded into SQL tables confirms all 15 instances including state code are correct.\n",
    "\n",
    "    SELECT value, COUNT(*) as num\n",
    "       FROM nodes_tags\n",
    "       WHERE key='state_code'\n",
    "       GROUP BY key;\n",
    "\n",
    "    NSW  15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML Data Primitives\n",
    "\n",
    "The code below explores the data further by giving a count of the data primitive included in the sydney_australia.osm file. The code checks the \"k\" value for each \"<tag>\" to identify potential problems. Four regular expressions check for certain patterns in the tags, and whether we have any tags with problem characters. \n",
    "\n",
    "The function 'key_type', provides a count of each of four tag categories in a dictionary. The four categories are \"lower\" for tags that contain only lowercase letters and are valid, \"lower_colon\" for otherwise valid tags with a colon in their names, \"problemchars\" for tags with problematic characters, and \"other\" for other tags that do not fall into the other three categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sydneyProcessing.py\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "   \n",
    "#Tag types, a count of each of four tag categories placed in a dictionary\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "                             \n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        print(element.get('k'))\n",
    "        if lower.search(element.get('k')):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif lower_colon.search(element.get('k')):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif problemchars.search(element.get('k')):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "            pprint.pprint(keys)\n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "def test():\n",
    "    keys = process_map('sydney_australia.osm')\n",
    "    pprint.pprint(keys)\n",
    "\n",
    "    \n",
    "#if __name__ == \"__main__\":\n",
    "#    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'problemchars': 8, 'lower': 745578, 'other': 8614, 'lower_colon': 104685}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV Conversion \n",
    "\n",
    "After the auditing and cleaning steps were performed, the OSM file was converted to CSV files for inclusion in a SQL database as tables. The script below details the conversion to CSV format, which can then be converted in SQL tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " %load 'schema.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saConversion.py\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"sydney_australia.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    p = 0\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for i in NODE_FIELDS:\n",
    "           node_attribs[i] = element.attrib[i]\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            node_tags_attribs = {}\n",
    "            temp = LOWER_COLON.search(tag.attrib['k'])\n",
    "            is_p = PROBLEMCHARS.search(tag.attrib['k'])\n",
    "            if is_p:\n",
    "               continue\n",
    "            elif temp:\n",
    "               split_char = temp.group(1)\n",
    "               split_index = tag.attrib['k'].index(split_char)\n",
    "               type1 = temp.group(1)\n",
    "               node_tags_attribs['id'] = element.attrib['id']\n",
    "               node_tags_attribs['key'] = tag.attrib['k'][split_index+2:]\n",
    "               node_tags_attribs['value'] = tag.attrib['v']\n",
    "               node_tags_attribs['type'] = tag.attrib['k'][:split_index+1]\n",
    "            else:\n",
    "               node_tags_attribs['id'] = element.attrib['id']\n",
    "               node_tags_attribs['key'] = tag.attrib['k']\n",
    "               node_tags_attribs['value'] = tag.attrib['v']\n",
    "               node_tags_attribs['type'] = 'regular'\n",
    "            tags.append(node_tags_attribs)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        id = element.attrib['id']\n",
    "        for i in WAY_FIELDS:\n",
    "            way_attribs[i] = element.attrib[i]\n",
    "        for i in element.iter('nd'):\n",
    "            d = {}\n",
    "            d['id'] = id\n",
    "            d['node_id'] = i.attrib['ref']\n",
    "            d['position'] = p\n",
    "            p+=1\n",
    "            way_nodes.append(d)\n",
    "        for c in element.iter('tag'):\n",
    "            temp = LOWER_COLON.search(c.attrib['k'])\n",
    "            is_p = PROBLEMCHARS.search(c.attrib['k'])\n",
    "            e = {}\n",
    "            if is_p:\n",
    "               continue\n",
    "            elif temp:\n",
    "               split_char = temp.group(1)\n",
    "               split_index = c.attrib['k'].index(split_char)\n",
    "               e['id'] = id\n",
    "               e['key'] = c.attrib['k'][split_index+2:]\n",
    "               e['type'] = c.attrib['k'][:split_index+1]\n",
    "               e['value'] = c.attrib['v']\n",
    "            else:\n",
    "               e['id'] = id\n",
    "               e['key'] = c.attrib['k']\n",
    "               e['type'] = 'regular'\n",
    "               e['value'] =  c.attrib['v']\n",
    "            tags.append(e)\n",
    "    \n",
    "    \n",
    "    return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(s_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(s_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview Statistics\n",
    "\n",
    "#### Size of Files\n",
    "    sydney_australia.osm .... 344.2 MB\n",
    "    sydneyAu.db ............. 188.8 MB\n",
    "    nodes.csv ............... 127.3 MB\n",
    "    nodes_tags.csv .......... 6.2 MB\n",
    "    ways.csv ................ 12.4 MB\n",
    "    ways_tags.csv ........... 23.4 MB\n",
    "    ways_nodes.cv ........... 44 MB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Unique Users\n",
    "\n",
    "    sqlite> SELECT COUNT(DISTINCT(e.uid))          \n",
    "    FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e;\n",
    "\n",
    "    count = 2382"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Unique Nodes \n",
    "    sqlite> SELECT COUNT(*) FROM nodes;\n",
    "\n",
    "    count = 1529440 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Unique Ways\n",
    "    sqlite> SELECT COUNT(*) FROM ways;\n",
    "\n",
    "    count = 208933\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Additional Nodes\n",
    "\n",
    "I chose to explore a number of additional node types to further the geographical exploration of the map data for consideration in the planning of a potential holiday trip. I started with a query into the top ten amenities Sydney has to offer. It is interesting to note the top amenity in Sydney is the communal bench. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Amenities:\n",
    "    sqlite> SELECT value, COUNT(*) as num\n",
    "       ...> FROM nodes_tags\n",
    "       ...> WHERE key='amenity'\n",
    "       ...> GROUP BY value\n",
    "       ...> ORDER BY num DESC\n",
    "       ...> LIMIT 10;\n",
    "       \n",
    "    bench             1465\n",
    "    restaurant        1015\n",
    "    cafe              884\n",
    "    drinking_water    836\n",
    "    parking           764\n",
    "    toilets           660\n",
    "    fast_food         604\n",
    "    bicycle_parking   588\n",
    "    post_box          423\n",
    "    place_of_worship  397\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everybody gets hungry. The next query provides a breakdown of the types of restaurants that call Sydney home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cuisine:\n",
    "    sqlite> SELECT nodes_tags.value, COUNT(*) as num\n",
    "       ...> FROM nodes_tags \n",
    "       ...>     JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='restaurant') i\n",
    "       ...>     ON nodes_tags.id=i.id\n",
    "       ...> WHERE nodes_tags.key='cuisine'\n",
    "       ...> GROUP BY nodes_tags.value\n",
    "       ...> ORDER BY num DESC\n",
    "       ...> LIMIT 20;\n",
    "       \n",
    "    thai                  74\n",
    "    chinese               64\n",
    "    italian               55\n",
    "    pizza                 45\n",
    "    japanese              41\n",
    "    indian                34\n",
    "    vietnamese            16\n",
    "    korean                15\n",
    "    greek                 12\n",
    "    sushi                 10\n",
    "    regional              9\n",
    "    asian                 8\n",
    "    international         8\n",
    "    malaysian             8\n",
    "    burger                7\n",
    "    asian;sushi;japanese  6\n",
    "    lebanese              6\n",
    "    seafood               6\n",
    "    fish_and_chips        5\n",
    "    french                5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This query explores data identifying religious structures, which could be used to identify potential architectural wonders worth visiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Liturgy, Top 3:\n",
    "    sqlite> SELECT nodes_tags.value, COUNT(*) as num\n",
    "       ...> FROM nodes_tags \n",
    "       ...>     JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value='place_of_worship') i\n",
    "       ...>     ON nodes_tags.id=i.id\n",
    "       ...> WHERE nodes_tags.key='religion'\n",
    "       ...> GROUP BY nodes_tags.value\n",
    "       ...> ORDER BY num DESC\n",
    "       ...> LIMIT 3;\n",
    "       \n",
    "    christian  338\n",
    "    buddhist   9\n",
    "    muslim     9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following query shows many different tourist attractions worth visiting. I'm impressed by the 327 viewpoints located throughout the city. Many of which provide glorious beach vistas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tourism, Top 10:\n",
    "    sqlite> SELECT nodes_tags.value, COUNT(*) as num\n",
    "       ...> FROM nodes_tags \n",
    "       ...>     JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE key='tourism') i\n",
    "       ...>     ON nodes_tags.id=i.id\n",
    "       ...> WHERE nodes_tags.key='tourism'\n",
    "       ...> GROUP BY nodes_tags.value\n",
    "       ...> ORDER BY num DESC\n",
    "       ...> LIMIT 10;\n",
    "       \n",
    "    information  341\n",
    "    viewpoint    327\n",
    "    picnic_site  232\n",
    "    hotel        130\n",
    "    attraction   123\n",
    "    artwork      121\n",
    "    museum       51\n",
    "    hostel       43\n",
    "    camp_site    21\n",
    "    motel        17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City art is explored in this query to provide some potential mid-point attractions while walking the city streets from one attraction to the next. City art also draws a nice contrast with artwork contained in museums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### City Art\n",
    "    sqlite> SELECT nodes_tags.value, COUNT(*) as num\n",
    "       ...> FROM nodes_tags \n",
    "       ...>     JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE key='artwork_type') i\n",
    "       ...>     ON nodes_tags.id=i.id\n",
    "       ...> WHERE nodes_tags.key='artwork_type'\n",
    "       ...> GROUP BY nodes_tags.value\n",
    "       ...> ORDER BY num DESC;\n",
    "       \n",
    "    sculpture     39\n",
    "    statue        20\n",
    "    mural         6\n",
    "    interactive   3\n",
    "    gates         1\n",
    "    graffiti      1\n",
    "    installation  1\n",
    "    sculptor      1\n",
    "    streetart     1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of exploring how the data is added to the map dataset I thought we could investigate the top contributing users. From the results below, we can see the top ten contributing users out of the 2382 total unique users. Based on these findings, user \"balcoath\" made 18.7% of the total contributions for the top ten users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Contributing Users\n",
    "\n",
    "    sqlite> SELECT e.user, COUNT(*) as num\n",
    "       ...> FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e\n",
    "       ...> GROUP BY e.user\n",
    "       ...> ORDER BY num DESC\n",
    "       ...> LIMIT 10;\n",
    "   \n",
    "    balcoath    117344\n",
    "    inas        89241\n",
    "    TheSwavu    74266\n",
    "    aharvey     66423\n",
    "    ChopStiR    60487\n",
    "    ozhiker2    51861\n",
    "    \"Leon K\"    46508\n",
    "    cleary      41601\n",
    "    Rhubarb     40735\n",
    "    AntBurnett  37524\n",
    "    \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Areas for Improvement\n",
    "\n",
    "Data integrity and the standardization of data is paramount in being able to perform reliable data analysis, and some of the more common errors in this dataset appear to be the result of human error. As is likely the case with the street name errors identified. Perhaps creating a data-entry template, Readme file, or a data type verification process via a java application could be utilize when users are manually entering data into the OpenStreetMap databases.\n",
    "\n",
    "This would likely prevent any mistakes before they are entered into the database, making the need to routinely audit and clean the data less cumbersome. However, some anticipated problems with this suggestion include the likelihood that some users wouldn't read the supporting materials or even ignore the instructions altogether. This might be avoided with solid and complete input from a GPS device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusions\n",
    "\n",
    "In the effort to reveal insights into the diversity of religious structures in the city, the \"places of worship\" value tag in the nodes_tag table was explored. For the purposes here, we could use these results to gain additional insight in the makeup of religious structures, and potentially identify some of the more popular religious structures worth visiting. \n",
    "\n",
    "While reviewing the results I couldn't help consider the lack of representation of those who don't affiliate with any religion. This is likely the result of this population not having a central place of worship. Gathering additional data in the proportional religious affiliation of the citizens of Sydney for comparison would provide a better glimpse into the religious makeup of the city. \n",
    "\n",
    "All the data explored in this project has provided some interesting results, which are sure to prove helpful in the planning of a future holiday to Sydney, Australia.   \n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
